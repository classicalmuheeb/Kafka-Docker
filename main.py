from mimesis import Person, Address, Payment, Food
from mimesis import locales
from kafka import KafkaProducer, KafkaConsumer
import json, time, logging
from pymongo import MongoClient
import concurrent.futures

"""
The data pipeline consists of two parts viz-a-viz:
    (i) Kafka-Elastic-Logstash-Kibana: Data generated by this Python script is used by a Kafka producer which is then 
      consumed by the ELK stack.
    (ii) Kafka-MongoDb: Same data generated is persisted into MongoDB
"""

"""Configure MongoDb connection"""
#Connect to MongoDB and Database
try:
    client = MongoClient('localhost', 27017)
    collection = client.testing_kafka.peoples_info
    print('Connection successful')
except:
    print('Oops! sorry, we couldnt connect')


person = Person(locale=locales.EN_GB)
address = Address(locale=locales.EN_GB)
payment = Payment()
food = Food()

def generate_person_information():
    return {
        "name": person.full_name(),
        "address": address.address(),
        "post_code": address.postal_code(),
        "age": person.age(),
        "email": person.email(),
        "height": person.height(minimum=1.5, maximum=2.0),
        "occupation": person.occupation(),
        "qualification": person.academic_degree(),
        "university": person.university(),
        "credit_card_number": payment.credit_card_number(),
        "credit_card_network":payment.credit_card_network(),
        "cvv": payment.cvv(),   
    }

def json_serializer(data):
    return json.dumps(data).encode("utf-8")

producer = KafkaProducer(bootstrap_servers=['localhost:29092'],
                         value_serializer=json_serializer)

#consume the produced Kafka Message and ingest into MongoDB
def consume_kafka_messages():
       consumer = KafkaConsumer('peoples_info', 
                                 bootstrap_servers=['localhost:29092'], #consume from the port 29092 expose from Kafka container in Docker 
                                 auto_offset_reset='earliest',
                                 group_id='peoples_info_group', #obtained by executing 'kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list' and 'kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group logstash'
                                                                #in the Kafka docker container.
                                 enable_auto_commit=True,
                                 #consumer_timeout_ms=10000,
                                 #fetch_max_wait_ms=50000,
                                 #max_poll_records=3000,
                                 #fetch_min_bytes=1000,
                                 #max_poll_interval_ms=500000,
                                 value_deserializer=lambda x: json.loads(x.decode('utf-8')))
       try:
            #Parse data received from Kafka
            for message in consumer:
                record = message.value
                name = record["name"]
                address = record["address"]
                post_code = record["post_code"]
                age = record["age"]
                email = record["email"]
                height = record["height"]
                occupation = record["occupation"]
                qualification = record["qualification"]
                university = record["university"]
                credit_card_number = record["credit_card_number"]
                credit_card_network = record["credit_card_network"]
                cvv = record["cvv"]      
            
                #create a dictionary of data and parse into 
                new_record = {
                        'name':name, 'address':address, 'post_code':post_code, 'age':age, 'email':email, 'height':height, 'occupation':occupation, 'qualification':qualification, 'university':university, 'credit_card_number':credit_card_number, 'credit_card_network':credit_card_network, 'cvv':cvv
                }
                collection.insert_one(new_record)

       except Exception as e:
             logging.error('Error encountered:', exc_info=True) 

if __name__ == "__main__":
    """
    Concurrency is being used because the function 'consume_kafka_messages()' is a blocking function, 
    and it will only continue to the next line when it receives a new message, which might not happen 
    if there are no new messages available.

    It will be executed in a separate thread, allowing the script to continue its execution 
    without waiting for Kafka messages indefinitely.

    Once done, it shuts down and release CPU resources.

    """
    with concurrent.futures.ThreadPoolExecutor() as executor:
        while True:
            person_info = generate_person_information()
            producer.send("peoples_info", person_info)
            producer.flush()
            
            # Start consuming Kafka messages in a separate thread
            future = executor.submit(consume_kafka_messages)
            print(person_info)
            time.sleep(10)